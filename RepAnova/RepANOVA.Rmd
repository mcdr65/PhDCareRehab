---
title: "Repeated Measures ANOVA"
author: "Andr√© Meichtry"
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
  html_document:
    number_sections: yes
    self_contained: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
---

```{r chunk_setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = T, out.width="49%",message=F, warning=F, comment=NA,
                      eval=T, cache.rebuild=F, cache=F, R.options=list(width=200,digits=3,show.signif.stars=FALSE),dev.args=list(bg = 'transparent'))
```


```{r needed,echo=TRUE,results="hide"}
library(lme4)
library(lmerTest)
library(psych)
library(ggplot2)
```





# Repeated measures ANOVA

## Model AMT (11.4.1)

Repeated Measures ANOVA with **one within-subject factor**.

$Y_{ij}=\mu+\alpha_j+\pi_i+\epsilon_{ij},\quad i=1,...,n;\quad j=1,...,I.$

* $\pi_i$ are subjects effects, they could be considered **fixed**, but
  most often, we will treat them as **random effects**, that is 
* $\pi_i \sim N(0,\nu^2)$ are **random intercepts** with **between-subject** variance $\nu^2$
* $\epsilon_{ij}\sim N(0,\tau^2)$ with **within-subject** variance $\tau^2$
* within-subject correlation
$\rho=Cor(Y_{ij},Y_{ik})=\frac{\nu^2}{\nu^2+\tau^2}$ for $j\neq k$. 
* $\sigma^2=\nu^2+\tau^2$
* This model is called a **Linear Mixed Model (LMM)**. In contrast to
linear models, they have **additional random part** to model the
**within-subject correlation**. $\rho$ is called the **intra-class correlation**.
* The advantage of treating the $\pi_i$ as random is that 
	+  we need less
  parameters (one between-subject variance $\nu^2$ instead of $n$
  parameters $\pi_i$)
	+ Fixed-effects parameters do not have interpretation as population parameters.



## Within-subject factor with 2 levels


The simplest Repeated Measures ANOVA is the **paired $t$-test** with $I=2$

<!-- Let us simulate some data with an R-function. **You need not to understand the code for simulation**. -->

```{r include=FALSE}
RepData<-function(n=30,I=2,mu=20,alpha=runif(I-1,mu/2,mu),nu=10,tau=5)
{
    N<-n*I ##observations
    set.seed(4)
    subject<-gl(n,I,N,labels=paste("s",1:n,sep=""))
    U<-rep(rnorm(n,0,nu),each=I)##random intercept
    E <- rnorm(N,0,tau)##random error
    time <- gl(I,1,N,labels=paste("t",1:I,sep=""))
    X<-model.matrix(~time)
    fixed <- c(mu,alpha) ##parameters (mu, alpha)
    response <- X%*%fixed+U+E ##systematic part + random intercept + random error
    data <- data.frame(subject,time,response)
    parameters<-c(n=n,I=I,mu=mu,alpha=alpha,nu=nu,tau=tau,rho=nu^2/(nu^2+tau^2))
    l<-list(data=data,parameters=parameters)
    return(l)
}    
```

### Example data


```{r include=FALSE}
tmp<-RepData() ##with default arguments
d.long2<-tmp$data
parms<-tmp$parameters
```


The data.frame `d.long2` consists of time points 1 and 2. 

<!-- The true -->
<!-- values are -->

```{r include=FALSE}
parms
```


```{r } 
headTail(d.long2)
aggregate(response~time,data=d.long2,summary)
describeBy(d.long2$response,group=d.long2$time,mat=TRUE,skew=FALSE)
with(d.long2,interaction.plot(time,subject,response))
``` 


A popular package for plotting is the **ggplot2** package:

```{r }
p <- ggplot(data = d.long2, aes(x = time, y = response, group = subject))
p <- p+geom_point()+geom_line()+stat_smooth(aes(group = 1),method="lm",se=FALSE)
p <- p + stat_summary(aes(group=1), geom = "point", fun.y = mean,shape = 17, size = 4)
p
```

### As paired $t$-Test
```{r }
t.test(response~time,paired=TRUE,data=d.long2)
cor(d.long2$response[d.long2$time=="t1"],d.long2$response[d.long2$time=="t2"])
```
### As one-sample $t$-Test changes
```{r }
x<-d.long2$response[d.long2$time=="t1"]
y<-d.long2$response[d.long2$time=="t2"]
t.test(y-x)
```

### Observed correlation
```{r }
cor(x,y)
```




### As ANOVA

**aov()** provides a wrapper to **lm()** for fitting linear
models. The main difference from lm is in the way print, summary and
so on handle the fit: this is expressed in the traditional
language of the analysis of variance rather than that of linear
models. If the formula contains a single Error term, this is used to
specify error strata, and appropriate models are fitted within
each error stratum.


```{r }
modelRep1<-aov(response~time+Error(subject),data=d.long2)
print(summary(modelRep1),digits=4)
```


### Repeat Sum of Squares...

Let us repeat the concept of **sum of squares** and reproduce the results above.

#### Model fits

```{r }
mod0 <- lm(response~1,d.long2) 
mods <- lm(response~subject,d.long2)
modt <- lm(response~time,d.long2)
modts <-lm(response~subject+time,d.long2)
```
#### Residual sum of squares

```{r }
rss.0 <- sum((mod0$residuals)^2)
#(ss.0<-sum((d.long2$response-mod0$fitted)^2)) ##equivalent...
rss.s <- sum((mods$residuals)^2)
rss.t <- sum((modt$residuals)^2)
rss.ts<- sum((modts$residuals)^2)
```

#### Explained Sum of Squares

```{r }
rss.0
rss.0-rss.s
rss.0-rss.t
rss.0-rss.ts
rss.ts
```


<!-- ### Fixed effects model -->

<!-- ```{r } -->
<!-- fixeff<-lm(response~subject+time-1,d.long2) -->
<!-- summary(fixeff) -->
<!-- sd(coef(fixeff)[1:parms[1]]) -->
<!-- ``` -->


### As Linear Mixed Model (LMM)

LMM are an alternative for the analysis of repeated
measurements for unbalanced data or data with missing values. We will
come back to LMM later. We use the **lmer()** function of the package **lme4** and
**lmerTest**. LMM are fitted using **Maximum Likelihood Estimation** (in
contrast to **lm()** and **aov()** which are fitted using **Least
Squares**). 

The syntax for the model is 

```{r }
lmm1<-lmer(response~time+(1|subject), data=d.long2)
summary(lmm1,cor=FALSE)
```
<!-- Compare to true parameter values: -->

The estimate of the intraclass correlation $\nu^2/\sigma^2$ is

```{r echo=FALSE}
v<- as.data.frame(VarCorr(lmm1))
v$percent<-v$vcov/sum(v$vcov)
v$percent[1]
```


```{r include=FALSE }
parms
```


```{r }
anova(lmm1)
plot(lmm1)
```





## Arbitrary number of levels

### Example data

The within-subject factor time now has $I=4$ levels:

```{r include=FALSE}
tmp<-RepData(n=50,I=4,mu=20,alpha=c(2,3,4),nu=10,tau=5)
d.long<-tmp$data
parms<-tmp$parameters
```

```{r include=FALSE}
parms
```

```{r }
headTail(d.long,7,7)
```

```{r fig.show="hold"}
aggregate(response~time,data=d.long,summary)
describeBy(d.long$response,group=d.long$time,mat=TRUE,skew=FALSE)


with(d.long,interaction.plot(time,subject,response))
p <- ggplot(data = d.long, aes(x = time, y = response, group = subject))
p <- p+geom_point()+geom_line()+stat_smooth(aes(group = 1),se=FALSE)
p <- p + stat_summary(aes(group=1), geom = "point", fun.y = mean,shape = 17, size = 4)
p
```

### As ANOVA

```{r }
modelRep2 <-aov(response~time+Error(subject),data=d.long)
summary(modelRep2)
```

### As LMM

```{r }
lmm2 <- lmer(response~time+(1|subject),data=d.long)
summary(lmm2,cor=FALSE)
anova(lmm2)
```
The estimate of the intraclass correlation $\nu^2/\sigma^2$ is

```{r echo=FALSE}
v<- as.data.frame(VarCorr(lmm2))
v$percent<-v$vcov/sum(v$vcov)
v$percent[1]
```

```{r }
plot(lmm2)
```





## Within- and between-subject factor

A frequent question is the changes of 2 groups from Pre to Post. This
corresponds to a model with one **within-subject factor time** and one
**between-subject factor group**:

$Y_{ijk}=\mu+\alpha_j\times\beta_k+\pi_i+\epsilon_{ijk},\quad
i=1,...,n\quad k=1,2\quad j=1,2.$ with 

+ $\alpha_j$ as time effects
+ $\beta_k$ as group effects
+ $\alpha_j:\beta_k$ as interaction effects. (=difference in slopes,
  effect of one predictor depends on the value on the other predictor.)


### Example data

<!-- Let us again simulate some data with an R-function. You need not to -->
<!-- understand the code for simulation! But if you are interested, you can -->
<!-- play with. -->

```{r include=FALSE}
RepData2<-function(n=100,mu=100,alpha=3,beta=5,gamma=0,nu=10,tau=5)
{
    N<-n*2 ##observations
    set.seed(65)
    subject<-gl(n,2,N,labels=paste("s",1:n,sep=""))
    U<-rep(rnorm(n,0,nu),each=2)##random intercept
    E <- rnorm(N,0,tau)##random error
    time <- gl(2,1,N,labels=paste("t",1:2,sep=""))
    group <- as.factor(c(rep("Ctr",n),rep("Trt",n)))
    X<-model.matrix(~time*group)
    fixed <- c(mu,alpha,beta,gamma) ##parameters (mu, alpha, beta, gamma)
    response <- X%*%fixed+U+E ##systematic part + random intercept + random error
    data <- data.frame(subject,time,group,response)
    parameters<-c(n=n,mu=mu,alpha=alpha,beta=beta,gamma=gamma,nu=nu,tau=tau,rho=nu^2/(nu^2+tau^2))
    l<-list(data=data,parameters=parameters)
    return(l)
}

tmp<-RepData2() ## with default arguments
d.longB<-tmp$data
parms<-tmp$parameters
```



```{r }
headTail(d.longB)
```

```{r fig.show="hold"}
with(d.longB,interaction.plot(time,group,response))
## with(d.longB,interaction.plot(time,subject,response))
p <- ggplot(data = d.longB, aes(x = time, y = response, group = subject))
p <- p + geom_line() + facet_grid(. ~ group)
p <- p + stat_smooth(aes(group = 1), method = "lm", se = FALSE) + stat_summary(aes(group = 1), geom = "point", fun.y = mean, shape = 17, size = 3)
p
```


```{r }
aggregate(response~time+group,data=d.longB,summary)
describeBy(d.longB$response,group=list(d.longB$time,d.longB$group),mat=TRUE,skew=FALSE)
tableone::CreateTableOne(vars="response",strata=c("group","time"),data=d.longB,test=FALSE)
```


### As ANOVA

```{r }
modelRep3 <-aov(response~time*group+Error(subject/time),data=d.longB) ##+Error(subject) is equivalent
print(summary(modelRep3),digits=4)
```


### As LMM

```{r }
lmm3 <- lmer(response~time*group+(1|subject),data=d.longB)
summary(lmm3,cor=FALSE)
anova(lmm3)
```


The estimate of the intraclass correlation $\nu^2/\sigma^2$ is


```{r echo=FALSE}
v<- as.data.frame(VarCorr(lmm3))
v$percent<-v$vcov/sum(v$vcov)
v$percent[1]
```

<!-- Compare to true values -->

```{r include=FALSE}
parms
```

### Fitted model* 

```{r }
predicted<-predict(lmm3)
p <- ggplot(data = d.longB, aes(x = time, y = predicted, group = subject))
p <- p + geom_line() + facet_grid(. ~ group)
p <- p + stat_smooth(aes(group = 1), method = "lm", se = FALSE) + stat_summary(aes(group = 1), geom = "point", fun.y = mean, shape = 17, size = 3)
p
```

### Residual analysis

```{r }
plot(lmm3)
```
