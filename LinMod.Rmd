---
title: "General Linear Model"
author: "Andr√© Meichtry"
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
  html_document:
    number_sections: yes
    self_contained: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
---



```{r chunk_setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = T, message=F, warning=F, comment=NA, autodep=F,
                      eval=T, cache.rebuild=F, cache=F, R.options=list(width=120,digits=2,show.signif.stars=FALSE,scipen=0),
                      out.width="49%", dev.args=list(bg = 'transparent'))
```

```{r }
library(psych)
```


# Analysis of covariance

We have looked at linear models with **categorical independent
variables**, often called ANOVA. Remember though that we looked at
ANOVA from a more global perspective, the perspective of model
comparison. 

Now, we look at a problem where we have one **continuous predictor**
in addition to a categorical predictor. Such models are also called **ANCOVA or analysis
of covariance**. 

In our journey, an ANCOVA is (just) one further special case of the General linear
Model (LM) which allows multiple categorical and continuous predictors.


## Statistical model

Remember that a model equation includes fixed and random quantities,

$\underbrace{Y_i}_{response}=\underbrace{\mu_i}_{deterministic}+\underbrace{\epsilon_i}_{stochastic}, \quad i=1,\ldots,n.$


Assume we have

+ one categorical predictor **group** with 2 values $A$ and $B$ 
+ one **continuous** predictor $X$. 
+ We then need 4 (5) parameters: **intercept** and **slope** for each group plus error
standard deviation.
  
+ The **Effects parameterization** of the interaction-effects model is


$Y_i=\underbrace{\beta_1+\beta_2
I_{B(i)}+\beta_3\,x_i+\beta_4\,x_i\,I_{B(i)}}_{\mu_i}+\epsilon_i$ with
indicator variable

$$
I_{B(i)} =
 \begin{cases}
 0, & B(i)=A \\
 1, & B(i)=B
 \end{cases} 
$$

The expectations (``Long run average''):

+  $E(Y_i; group_i=A,X_i=\,0)=\beta_1$
+  $E(Y_i; group_i=B,X_i=\,0)=\beta_1+\beta_2$
+  $E(Y_i; group_i=A,X_i=x_i)=\beta_1+\beta_3x_i$
+  $E(Y_i; group_i=B,X_i=x_i)=\beta_1+\beta_2+(\beta_3+\beta_4)x_i$

The quantity of interest are the parameters of the model, i.e. $\beta_4$, the **difference in
slopes between groups**.

		
## Simulation of data*

It is not mandatory to understand the data simulation code. But there
is considerable conceptual value in studying the code if you have time.


```{r }
set.seed(10)  
n.groups <- 2
n.sample <- 30
n <- n.groups*n.sample ##sample size
ind <- rep(1:n.groups, each=n.sample) ##Indicator for group
group <- factor(ind, labels = c("A", "B"))
height <- rnorm(n, mean=165, sd=11.4)
covariates<-data.frame(group,height)
Xeffects <- model.matrix(~group*height)
Xmeans <- model.matrix(~group*height-height-1)
sigma <-2
betaMeans <- c(muA<--36.475,muB<--45.5,slopeA<-0.615,slopeB<-0.7)
betaEffects <- c(muA,muB-muA,slopeA,slopeB-slopeA)
lin.pred <- Xeffects %*% betaEffects 	
lin.pred2 <- Xmeans %*% betaMeans  
#all.equal(lin.pred,lin.pred2) ## should be same of course
eps <- rnorm(n = n, mean = 0, sd = sigma) ## add noise
weight <- lin.pred + eps ## response
df <- data.frame(group,height,weight)
```


## Data

```{r }
str(df)
headTail(df)
plot(weight~height,data=df,col=as.numeric(group))
legend(140,80,legend=levels(group),col=c(1,2),pch=21)
```


## Analysis

### Description

```{r }
by(df[,-1],df$group,describe) 
cor(weight,height,method="pearson")
cor(weight,height,method="spearman")
by(df[,-1],df$group,cor)
```


### Fitting a linear model

We now fit a linear model to the data, of course again with `lm()`

```{r }
mod<-lm(weight~group*height,df)
mod
```

The output show the point estimates $\hat{\beta}$. For further
information, we already know the `summary()` function.

```{r }
summary(mod)
```
Note that the term **residual standard error** is a misnomer with a long
tradition, since *standard error* for an estimated parameter $\theta$
usually means $\sqrt{Var(\hat{\theta})}$. The correct term would be
**residual standard deviation**. 

The true beta values and the true sigma are

```{r }
betaEffects
sigma
```
Of course, in real life, you will not know the true values.

We can now add the model fit to the data:
```{r }
plot(weight~height,data=df,col=as.numeric(group))
legend(150,80,legend=levels(group),col=c(1,2),pch=21)
abline(mod$coef[1],mod$coef[3],col=1)
abline(mod$coef[1]+mod$coef[2],mod$coef[3]+mod$coef[4],col=2)
```


Let us construct confidence intervals for the true effects.
```{r }
confint(mod,level=0.95)
```
$\beta$ will be overlapped by a ($1-\alpha$)-CI with a "long-run-probability" of ($1-\alpha$).

If we want a enhanced output, we can use the `kable()` function:
```{r }
knitr::kable(cbind(summary(mod)$coef,confint(mod)),digits=3)
```

and for the `anova()` results
```{r }
knitr::kable(anova(mod),digits=3)
```

### Interpretation

The estimated **increase in weight by unit change on height** is larger
in group B than in group A, the **difference in slopes** is
$\hat{\beta}=`r mod$coef[4]`$ (with 95% CI: `r confint(mod)[4,]`). We can reject the null model of no-interaction.

### Principle of marginality

**Take care**: $F$-tests in R (`anova()` and `aov()`) are
**sequential** (so-called Type I sum of squares), the order the terms enter
the model does matter! The $t$-tests in `lm()` on the contrary are marginal (impact of the
variables, given the presence of **all the other variables** in the
model).

Note the difference between the summary output and the anova
output. In the former, we see marginal tests, in the latter, we see --
the correct -- sequential tests.
Sequential tests (also called Type I tests) respect the the principle of [**marginality**](https://en.wikipedia.org/wiki/Principle_of_marginality). As
example, it makes no sense that a main effect is controlled for the
interaction effect. **Do not interpret main effects in the presence of
interaction effects!** 


If we wanted to reproduce the senseless $p$-value for
the group main effect in the summary output (`r summary(mod)$coef[2,4]`), we compare the interaction model with the
same model without the main effect of group:

```{r }
modNoMainG<-lm(weight~height*group-group) #equals weight~height+height:group
anova(mod,modNoMainG)
```
Marginal tests (often used in SPSS, so called Type III tests) are implemented also in the
`Anova()` function of the `car` package,
but **in general, you are more safe with sequential tests**.
```{r }
car::Anova(mod,type=3)
```

### Global $F$-test

The global $F$-test in the summary output tells us if the models explains
anything at all, this can be reproduced by

```{r }
mod0<-lm(weight~1)
anova(mod0,mod)
```


### Residual analysis

```{r }
plot(mod,which=1)
```

## Prediction

At the end of the day, we also want to predict new observations from a fitted
model. This is implemented in the `predict()` function. There are two
kinds of predictions, mean predictions or individual predictions.

The difference between **confidence bound for the expected value** given
predictor value versus **prediction bound for future observation**
given predictor value is visualized in [shinyApp](https://rstudio-pubs.zhaw.ch/connect/#/apps/8/access).


Assume we want to predict new observations for different arbitrary combinations
on group and height:

```{r }
new<-data.frame(group=c("A","B","A"),height=c(170,180,190))
new
```


+ Uncertainty for new observation $Y_{new}\mid X_{new}=x_{new}$
```{r }
pred<-predict(mod,newdata=new,interval="prediction")
cbind(new,pred)
```

+ Uncertainty for the conditional mean $E(Y_{new}\mid X_{new}=x_{new})$
```{r }
pred2<-predict(mod,newdata=new,interval="confidence")
cbind(new,pred2)
```

Let us draw confidence and prediction bounds for each possible length
and for each group:

```{r fig.show="hold"}
pred.frame<-data.frame(group="A",height=seq(min(df$height),max(df$height)))
pc<-data.frame(predict(mod,newdata=pred.frame,interval="confidence"))
pp<-data.frame(predict(mod,newdata=pred.frame,interval="prediction"))
plot(pred.frame$height,pc[,1],col=2,type="l",xlab="height",ylab="predicted weight",main="Group A")
lines(pred.frame$height,pc[,2])
lines(pred.frame$height,pc[,3])
lines(pred.frame$height,pp[,2],lty=2)
lines(pred.frame$height,pp[,3],lty=2)
pred.frame<-data.frame(group="B",height=seq(min(df$height),max(df$height)))
pc<-data.frame(predict(mod,newdata=pred.frame,interval="confidence"))
pp<-data.frame(predict(mod,newdata=pred.frame,interval="prediction"))
plot(pred.frame$height,pc[,1],col=2,type="l",xlab="height",ylab="predicted weight",main="Group B")
lines(pred.frame$height,pc[,2])
lines(pred.frame$height,pc[,3])
lines(pred.frame$height,pp[,2],lty=2)
lines(pred.frame$height,pp[,3],lty=2)

```

Of course, predictions for a new observation (dashed lines) have more uncertainty (larger
prediction bounds) than predictions of an expected value (solid
lines, called confidence bounds).

**Uncertainty of predictions can be (very) large**. In statistics, it
is crucial to quantify the corresponding uncertainty. We have seen in
this section that we cannot only quantify uncertainty for
estimates of parameters $\beta$, but also for future unobserved
responses $\hat{Y}$! 

As example, assume a study with running athletes to predict
Maximal Heart Rate as a function of age with data.

```{r }
Age <- c(18,23,25,35,65,54,34,56,72,19,23,42,18,39,37)
HR <- c(202,186,187,180,156,169,174,172,153,199,193,174,198,183,178)
```

```{r }
modHR <- lm(HR~Age)
pred.frame<-data.frame(Age=18:72)
pp<-predict(modHR,interval="prediction",newdata=pred.frame)
pc<-predict(modHR,interval="confidence",newdata=pred.frame)
pred.age<-pred.frame$Age
plot(pred.age,pp[,1],lty=1,type="l",main="Prediction of Maximal Heart Rate",ylab="predicted",col=2,xlab="age")
lines(pred.age,pc[,2],lty=1)
lines(pred.age,pc[,3],lty=1)
lines(pred.age,pp[,2],lty=2)
lines(pred.age,pp[,3],lty=2)
grid()
```

We see that the uncertainty is high and that simple formulas (such as 220-age, etc.)
do not work for most people.
