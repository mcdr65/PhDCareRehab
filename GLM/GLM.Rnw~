%% \documentclass[11pt]{article}
%%\usepackage{beamerarticle}
\documentclass[extsize,handout,10pt]{beamer}
\usepackage{preambBeamer} %%preambBeamer for presentation (no hyperref, etc.)
\usepackage{texab} %%Abkürzungen

\mode<article>{\usepackage{}}
\mode<presentation>{\usetheme{} \usecolortheme{}}
\setbeamertemplate{footline}[frame number]


<<include=FALSE>>=
library(knitr)
<<setup,include=FALSE>>=
opts_chunk$set(dev="pdf",echo=TRUE,fig.path="figures/GesWiss2",fig.align="center",
               background="transparent",out.width=".49\\linewidth",warning=FALSE,
               tidy=FALSE,size="tiny",R.options=list(digits=7,scipen=999,show.signif.stars=FALSE))
knit_theme$get()##choose a themeBERATUNG/
knit_theme$set("fruit")
options(width=140)
library(carData)
library(psych)
library(car)
#data(package="carData")
@ 

%\pgfpagesuselayout{4 on 1}[a4paper,border shrink=5mm, landscape]
\bibliographystyle{alpha}


\title{Introduction to the Generalized Linear Model}
\subtitle{Logistic and Poisson Regression}
\author{André Meichtry}
\institute{\GE \\ \ZHAWE}
\date{2020}
%\titlegraphic{\includegraphics[width=.2\textwidth]{/home/meichtry/BERATUNG/zhawD.jpg}}


\begin{document}

\selectlanguage{english}



\maketitle
%\frame{\tableofcontents}





\begin{frame}[fragile]
  \frametitle{Generalized Linear Model (GLM)}
\begin{itemize}
\item We want to generalize the linear model to discrete or continuous
  outcomes
\item Dichotomous event outcome, leading to \alert{Logistic regression}
\item Counts as outcome, leading to \alert{Poisson regression}
\end{itemize}
<<echo=FALSE>>=
N<-100
x<-sort(runif(N,-4,4))
alpha<-0
beta<-1
mui<-alpha+x*beta
pii<-exp(mui)/(1+exp(mui))
Y <-rbinom(N,size=1,prob=pii)
mod<-glm(Y~x,family="binomial")
pred<-predict(mod,type="response",se.fit=TRUE)
plot(x,Y,main="Logistic regression")
lines(x,pred$fit)
@

\end{frame}


\begin{frame}
  \frametitle{Aspects of generalization}
  \begin{itemize}
  \item Link function
  \item Variance function
  \item Distribution of the exponential family
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Link function}
  \begin{itemize}
  \item \alert{Systematic part:} The expectation of the response,
    $\mu_i=\Erw(Y_i)$, is transformed with a \alert{link function}.
  \item This quantity is a linear function of the parameter $\beta_j$,
    called the \alert{linear predictor} $\eta_i$ with the link function
    $h(\cdot)$
    \begin{equation}
      \label{eq:55}
      \boxed{h(\Erw(Y_i))=h(\mu_i)=\eta_i=\bmath{x}^T_i\bmath{\beta}.}
    \end{equation}
  \end{itemize}
\begin{itemize}
\item Usual Link functions
\begin{itemize}
  \item Linear regression: Identity function
  \item Logistic regression: logit function
  \item Poisson regression: log function
  \end{itemize}
  \end{itemize}
\end{frame}



\begin{frame}
  \frametitle{Variance function}
  \begin{itemize}
  \item \alert{Random part:} The variance $\Var(Y_i)$ is a function of
    the expectation,
    \begin{equation}
      \label{eq:56}
      \boxed{\Var(Y_i)=\phi v(\mu_i),}
    \end{equation}
    where $v(\cdot)$ is the \alert{variance function} and $\phi$ is the
    \alert{dispersion parameter}, which has to be estimated or not.
  \end{itemize}
\begin{itemize}
\item Usual variance functions
\begin{itemize}
  \item Linear regression: $v(\mu_i)=1$ with $\phi=\sigma^2$
  \item Logistic regression: $v(\mu_i)=\mu_i(1-\mu_i)$ and $\phi=1$
  \item Poisson regression: $v(\mu_i)=\mu_i$ and $\phi=1$
  \end{itemize}
  \end{itemize}
\end{frame}



\begin{frame}
  \frametitle{Distributions}
  \begin{itemize}
  \item Each class of a GLM follow a model with density of the
    \alert{exponential family}\footnote{\tiny distributions of the
      exponential family have densities 
      $f(y_i)=\exp\{(y_i\theta_{i}-A(\theta_i))/\phi+B(y_i,\phi)\}$
      with $\theta_{i}$ as the \alert{canonical parameter} with ca be
      expressed trough $\mu_i$. One can show that
      $\Erw(Y_i)=\mu_i=A'(\theta_{i})$ and
      $\Var(Y_i)=A''(\theta_i)\phi$. The function $A(\cdot)$ fixes the
      exponential family, $B(\cdot)$ is a normalizing function.}. Special cases and most often used are:
    \item The \alert{Normal distribution}
    \item The \alert{Binomial distribution}
    \item The \alert{Poisson distribution}
  \end{itemize}

\end{frame}


\begin{frame}
  \frametitle{Recap: Linear Model}
  \begin{itemize}
  \item $Y_i\sim \N(\bmath{x}^T_i\bmath{\beta},\sigma^2)$.
  \item The model can be written:
    $Y_i=\bmath{x}^T_i\bmath{\beta}+\epsilon_i$. 
  \item The expectation
    $\mu_i$ is
    \begin{equation}
      \label{eq:54}
      \mu_i=\Erw(Y_i)=\bmath{x}^T_i\bmath{\beta}.
    \end{equation}
  \item The link function $h(\cdot)$ is the identity and the
    variance function is $v(\mu_i)=1$, the dispersion parameter is
    known, $\phi=\sigma^2$.
  \item Interpretation: $\beta_j$ is the difference in expectations
    for two subpopulations that differ on $x_j$ by on unit (slope).
  \end{itemize}
\end{frame}




\begin{frame}[fragile]
  \frametitle{Recap: Linear Model for Fertility}
  \begin{itemize}
  \item Estimation with least squares:  \texttt{lm()} 
  \end{itemize}

<<>>=
modlm<-lm(Fertility~.,swiss)
modlm0<-lm(Fertility~1,swiss) ##for later
summary(modlm)
@ 
\end{frame}




\begin{frame}[fragile]
  \frametitle{The same model as GLM}

    \begin{itemize}
    \item Estimation with maximum likelihood: \texttt{glm()}
    \end{itemize}

<<>>=
modglm<-glm(Fertility~.,swiss,family=gaussian)
modglm0<-glm(Fertility~1,swiss,family=gaussian)## for later
summary(modglm)
@ 
\end{frame}

\begin{frame}[fragile]
  \frametitle{What is different?}


    \begin{itemize}
    \item ``Deviance'' versus Sum of Squares
    \item ``Likelihood ratio tests'' versus $F$-tests
    \item Explanations in next slides
    \end{itemize}

<<>>=
anova(modglm,modglm0,test="LR")
anova(modlm,modlm0,test="F")
@ 
\end{frame}




\begin{frame}
  \frametitle{Estimation and Tests}
  \begin{itemize}
  \item Estimation via \alert{Maximum Likelihood Principle} 
  \item We look at the (logarithmic) probability of the data \alert{as function of the
    parameters} and search the maximum.
  \item The log-likelihood $l$ is
    \begin{equation}
      \label{eq:1}
      l(\bmath{\beta})=\sum_{i=1}^n \log \Pr(Y_i=y_i \mid \bmath{x}_i,\bmath{\beta})
    \end{equation}
  \item The $\bmath{\beta}$ that maximizes $l(\bmath{\beta})$ is called the
    \alert{Maximum Likelihood Estimate (MLE)} $\hat{\bmath{\beta}}$
  \item One can show that the MLE has an asymptotic normal
    distribution.
  \end{itemize}
\end{frame}



\begin{frame}
  \frametitle{Estimation and Tests}
  \begin{itemize}
  \item \alert{Residual Deviance} replaces the \alert{residual sum of squares} 
    and is defined as
    \begin{equation}
      \label{eq:2}
      D=2(l_{max}-l(\hat{\bmath{\beta}}))
    \end{equation}
  \item $l_{max}$ is the likelihood for the ``maximal'', the saturated
    model (one parameter for each observation $i$)
  \item \alert{Null Deviance} replaces the \alert{total sum of
      squares}
\begin{equation}
      \label{eq:2}
      D=2(l_{max}-l_0)
    \end{equation}
\end{itemize}
\end{frame}




\begin{frame}
  \frametitle{Estimation and Tests}
  \begin{itemize}
  \item \alert{Likelihood-Ratio-Test:} The difference in deviance 
    \begin{equation}
      \label{eq:3}
      2(l_{Large}-l_{Small})
    \end{equation}
  \item has an asymptotic chi-square distribution 
  \item with the difference of the number of parameters as degrees of
    freedom.
\end{itemize}
\begin{alertblock}

\begin{itemize}
\item $H_0$: Modell small with $p_{Small}$ parameters is true.
\item $H_1$: Modell large with $p_{Large}>p_{Small}$ parameters is
  true.
\item $2(l_{Large}-l_{Small})\overset{approx}{\sim} \chi^2_{p_{Large}-p_{Small}}$
\end{itemize}
\end{alertblock}

\end{frame}


\begin{frame}
  \frametitle{Logistic regression}
  \begin{itemize}
  \item The distribution of the $Y_i$ is binomial,
    $$Y_i\sim \Bin{\mu_i=\pi_i,n=1}$$
  \item The model for $\mu_i=\pi_i$ can be written
    \begin{equation}
      \label{eq:57}
      \logit(\pi_i)=\bmath{x}^T_i\bmath{\beta},
    \end{equation}
  \item The link function is
    $h(\pi_i)=\logit(\pi_i)=\log(\pi_i/(1-\pi_i))=\log \mathrm{odds}$
  \item The variance function $v(\pi)=\pi(1-\pi)$ and $\phi=1$.
  \item Interpretation: $\beta_j$ (except for the intercept) is the difference in logits (\alert{log
    odds ratio}) for two subpopulations that differ on $x_j$ by on unit.
  \item $\exp(\beta_j)$ (except for the intercept) is the \alert{odds
      ratio} $OR$ for the event for two subpopulations that differ on $x_j$ by one
    unit.
  \end{itemize}
\end{frame}


\begin{frame}[fragile]
  \frametitle{Example with one continuous predictor}

<<echo=FALSE>>=
set.seed(10)  
N<-30
x<-sort(runif(N,-5,5))
alpha<-0
beta<-1
mui<-alpha+x*beta
pii<-exp(mui)/(1+exp(mui))
Y<-rbinom(N,size=1,prob=pii)
@

<<>>=
headTail(cbind(x,Y))
@ 

<<>>=
mod0<-glm(Y~1,family="binomial")
mod<-glm(Y~x,family="binomial")
summary(mod)
@

\end{frame}



\begin{frame}[fragile]
  \frametitle{Wald-tests and LRT-Tests}

  \begin{itemize}
  \item Tests of individual coefficients based on approximative normality are
    called \alert{Wald}-tests. 
  \item Crude assumption about the
    shape of the likelihood. 
  \item The LRT takes the likelihood values as they are. 
  \item Therefore LR-tests are usually superior to Wald-tests
  \item They are asymptotically equivalent.
  \item \texttt{confint()} constructs LR confidence intervals if a
    glm-object is given as argument.
  \end{itemize}

<<>>=
anova(mod,mod0,test="LR")
confint(mod)
@

<<include=FALSE>>=
lmtest::waldtest(mod,mod0,test="Chisq") ## based on approximative normality, equal z-test of the coef.
lmtest::lrtest(mod,mod0) ## Likelihood ratio test
@


\end{frame}


\begin{frame}[fragile]
  \frametitle{Model and data} 

<<fig.show="hold">>=
pred<-predict(mod,type="response")
plot(x,Y)
lines(x,pred)
@
 

\end{frame}




\begin{frame}[fragile]
  \frametitle{Residual analysis} 
What residuals are is not unambiguous:
  \begin{itemize}
  \item Raw residuals (Response residuals) $R_i=Y_i-\hat{\pi_i}$
  \item Working residuals (transformed on the space of the linear predictor)
  \item Deviance residuals: $\sign(Y_i-\hat{\pi_i})\cdot\sqrt{d_i}$ with
    $d_i$ as the contribution $i$ to the deviance\footnote{would be equal to the square root of
    a squared residual in normal distribution.}.
  \item Pearson residuals (Raw residuals divided by the standard
    deviation)
  \end{itemize}
 

\end{frame}



\begin{frame}[fragile]
  \frametitle{Residual analysis} 

  \begin{itemize}
  \item Working residuals against linear predictor
  \item Response residuals against fitted values
  \end{itemize}

<<>>=
plot(mod,which=1)
@
 

\end{frame}




\begin{frame}[fragile]
  \frametitle{Example 2: HIV} 
<<>>=
d.hiv<-read.csv("https://raw.githubusercontent.com/mcdr65/PhDCareRehab/master/Data/HIV.csv")
#str(d.hiv)
f<-1:11
d.hiv[,f]<-lapply(d.hiv[,f],as.factor)
@

<<>>=
head(d.hiv)
summary(d.hiv)
@
 

\end{frame}



\begin{frame}[fragile]
  \frametitle{Example 2: Marginal Wald tests} 



<<>>=
fit <- glm(aids~age3+gender+race3+educ4+employment+disability+dep+paindic, family="binomial",data=d.hiv)
summary(fit)
@


\end{frame}




\begin{frame}[fragile]
  \frametitle{Example 2: Sequential LR tests} 

<<>>=
anova(fit,test="LR")
@
 
\begin{itemize}
\item One could proceed with different model comparisons.
\end{itemize}

\end{frame}






\begin{frame}[fragile]
  \frametitle{Example 2: Tukey-Anscombe Plot} 

<<>>=
plot(fit,which=1)
@
 

\end{frame}



















\begin{frame}
  \frametitle{Poisson regression}
  \begin{itemize}
  \item Model for counts $Y_i$
  \item The distribution of the $Y_i$ is Poisson, $Y_i \sim \Pois{\mu_i}$ with expectation $\mu_i$.
  \item The model is
  \begin{equation}
  \label{eq:4}
  \log(\mu_i)=\bmath{x}^T_i\bmath{\beta}
  \end{equation}
  \item The link function is $h(\mu_i)=\log(\mu_i)$
                                            \item The variance
  function is $v(\mu_i)=\mu_i$ und $\phi=1$.
  \item $\beta_j$ is the difference in the logs of expected counts 
  \item $\exp(\beta_j)$ is the (risk, rate, count) ratio for two
  subpopulations that differ on $x_j$ by on unit (except for the
                                                  intercept.)
\end{itemize}
\end{frame}


\end{document}

