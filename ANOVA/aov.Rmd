
---
title: "Analysis of variance ANOVA"
author: "André Meichtry"
output:
  html_document:
    number_sections: yes
    self_contained: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    toc_depth: '3'
---


```{r chunk_setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = T, out.width="49%",message=F, warning=F, comment=NA,
                      eval=T, cache.rebuild=F, cache=F, R.options=list(digits=5,show.signif.stars=FALSE),dev.args=list(bg = 'transparent'))
```


```{r needed,echo=TRUE,results="hide"}
library(psych)
library(emmeans)
```




# Univariate one-factorial ANOVA

## Statistical model

1. Means parameterization: 
	+ $Y_{ij}=\mu_i+\epsilon_{ij},  \quad i=1,...,I;  \quad j=1,...,n_i;
      \quad \epsilon_{ij}\overset{i.i.d.}{\sim} N(0,\sigma^2)$ (or with $\epsilon_i$  i.i.d. and “large” sample size) 
    + $Y_{ij}\overset{i.i.d.}{\sim} N(\mu_i,\sigma^2)$ (equivalent)
2. Effects parameterization: 
	+ $Y_{ij}=\mu+\alpha_i+\epsilon_{ij}$
	+ $Y_{ij}\overset{i.i.d.}{\sim} N(\mu+\alpha_i,\sigma^2)$ (equivalent)



## Example Data

We reproduce the example in AMT, page 99. 


```{r }
Bef <- c(20, 12, 18, 14, 16, 21, 17, 13, 18, 21, 13, 12, 15, 17, 16, 17,  9, 10, 15,  8,  8, 11, 13, 14)
treat<-as.factor(c(rep("MP",10),rep("MA",6),rep("M",8)))
data<-data.frame(Bef,treat)
headTail(data)
str(data)
```

```{r }
boxplot(Bef~treat,data=data)
describeBy(data,group=data$treat,mat=FALSE)
```


## ANOVA as model comparison

* We will look at ANOVAS from the perspective of **model comparison**. 
* Unrestricted model: $\mu_1,\mu_2,\mu_3$ have to be estimated:

```{r }
mod  <- lm(Bef~treat,data) 
```

* Restricted model: $\mu_1=\mu_2=\mu_3$, or $\alpha_i=0$. There remains only one
 parameter, the overall intercept $\mu$: $Y_{i}=\mu+\epsilon_{i}$

```{r }
mod0 <- lm(Bef~1,data)
```
* Test $\alpha_i=0,i=1,2,3$

```{r }
anova(mod0,mod)
```


## Calculations "by hand"

### Sum of squares

```{r }
(RSS <-sum(residuals(mod)^2))
(SS  <-sum(residuals(mod0)^2))
((SS-RSS)/SS) ##known as R^2
```

### Degrees of freedom
```{r}
(df2<-mod$df.residual) ##numerator df
(df1<-mod0$df.residual-mod$df.residual) ##denominator df
```

### $F$-test
The $F$-statistic is the **amount of available fit that is actually achieved**, that is

$$F=\frac{(SS-RSS)/df_1}{RSS/df_2}=\frac{MS_{explained}}{MS_{error}}$$



```{r}
F <- (SS-RSS)/(df1)/(RSS/(df2))
p <- 1-pf(F,df1=df1,df2=df2)
sigma.mod <- sqrt(RSS/df2)
print(data.frame(SS,RSS,ESS=SS-RSS,F,p,sigma.mod),row.names=FALSE)
```


## Estimates

```{r }
summary(mod)
confint(mod)
#summary(mod0) ## The only-intercept model, uncomment if you want to look at it.
```

## Contrasts

We want to estimate $\mu_1-\mu_2$, etc.

```{r }
emmeans(mod,specs=pairwise~treat,infer=TRUE) ##Estimated marginal means
```


## Check assumptions
Our data were simulated from an ANOVA-modell, so the assumptions are met.

In the analysis stage, however, we always have the check the assumption of
**homogeneity of variance** (and normality, but less important).

```{r fig.show="hold"}
plot(mod,which=c(1,2))
```


## Classical version

`aov()` function in R:

```{r }
modc <-aov(Bef~treat,data)
summary(modc)
TukeyHSD(modc)
plot(TukeyHSD(modc))
```



# Two-factorial ANOVA



## Statistical model

 
2. Effects parameterization: 
	+ $Y_{ijk}=\mu+\alpha_i+\beta_j+(\alpha\beta)_{ij}+\epsilon_{ijk}, \quad
i=1,\ldots,I,\quad j=1,\ldots,J, \quad k=1,\ldots,n_{ij}, \quad
\epsilon_{ijk}\overset{i.i.d.}{\sim} N(0,\sigma^2)$.
	+ $Y_{ijk}\overset{i.i.d.}{\sim} N(\mu+\alpha_i+\beta_j+(\alpha\beta)_{ij},\sigma^2)$ (equivalent)

2. Means parameterization: 
	+ $Y_{ijk}=\mu_{ij}+\epsilon_{ijk}$
    + $Y_{ijk}\overset{i.i.d.}{\sim} N(\mu_{ij},\sigma^2)$ (equivalent)


## Simulate some data

We simulate some cross-sectional data. You do not need to understand
the code.

```{r }
nage <- 3
ntherapy <- 2
nsample <- 100
n <- nage * nsample* ntherapy
age <- gl(n = nage, k = nsample, length = n,labels=c("child","young","old"))
therapy <- gl(n = ntherapy, k = nsample, length = n,labels=c("Ctrl","Trt"))
mu <- 40
alpha <- c(1, 1) 
beta <- c(1)
gamma <- c(-3,3)
parameter <- c(mu, alpha, beta, gamma)
sigma <- 12
set.seed(9)
eps <- rnorm(n, 0, sigma)
X <- as.matrix(model.matrix(~ age*therapy) ) 
response <- as.numeric(as.matrix(X) %*% as.matrix(parameter) + eps)
d.cross<-data.frame(response,age,therapy)
```

```{r }
headTail(d.cross)
```

## Two-factorial model 

We begin with a cross-sectional analysis, a two-factorial *balanced*
design:

```{r }
with(d.cross,xtabs(~age+therapy))
```
```{r }
aggregate(response~therapy+age,data=d.cross,summary)
with(d.cross,interaction.plot(x.factor=age,trace.factor=therapy,response=response,trace.label="treatment",xlab="age group",ylab="mean of response"))
```


```{r }
model2f <-lm(response~age*therapy,data=d.cross)
summary(model2f)
```
Compare to true values

```{r }
parameter
```

```{r }
anova(model2f)
summary(aov(response~age*therapy,data=d.cross)) ## equivalent!
```

## Sequential versus marginal effects
**Take care**: $F$-tests in R (`anova()` and `aov()`) are
**sequential** (so-called Type I sum of squares), the order the terms enter
the model does matter! The $t$-tests in `lm()` on the contrary are marginal (impact of the
variables, given the presence of all the other variables in the
model). This is important when the design is unbalanced. (This is not the case
in our example, the results do not differ). Let us change the order.

```{r }
summary(aov(response~therapy*age,data=d.cross)) ## equivalent!
```
If one needs so-called Type III sum of squares (marginal effects), you
have to use the `Anova()` function of the package **car**. 

```{r }
library(car)
Anova(model2f,type=3)
```
