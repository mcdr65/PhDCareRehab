---
title: "One-sample $t$-test as a model comparison"
author: "Andr√© Meichtry"
output:
  html_document:
    number_sections: yes 
    self_contained: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
---


```{r chunk_setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = T, out.width="49%",message=F, warning=F, comment=NA,
                      eval=T, cache.rebuild=F, cache=F, R.options=list(digits=5,show.signif.stars=FALSE),dev.args=list(bg = 'transparent'))
```






# Simulation of data with parameters $\mu$ and $\sigma$ assumed known

Simulate some two-group-data from model with parameters $\mu_i$ and $\sigma$ assumed known:

* Parameterization: $Y_{i}=\mu+\epsilon_{i}, i=1,...,n_i$, $\boxed{Y_{i}\sim N(\mu,\sigma^2)}$




```{r}
n <- 20
mu <- 2
sigma <-10
set.seed(9)
X <- rnorm(n,mu,sigma)
X
summary(X)
boxplot(X,horizontal=TRUE)
```


## Models

### Unconstrained, unrestricted or full model
$\mu$ is unknown and has to be estimated: $$\boxed{X_i\sim N(\mu,\sigma^2)}$$
```{r}
modU <- lm(X~1) ##regression of X on only intercept
```


### Constrained or restricted model
$\mu$ is assumed known and has not to be estimated, i.e. $$\boxed{H_0: \mu=0, \quad X_i\sim N(0,\sigma^2)}$$
```{r}
modR <- lm(X~0) ##formula for excluding the intercept
```



### in R with ANOVA for model comparison
```{r}
anova(modR,modU)
```




## By hand

### Residual sum of squares and explained sum of squares
```{r}
(RSSU <-sum(modU$residuals^2))
(RSSR <-sum(modR$residuals^2))
(RSSR-RSSU)
```

### degrees of freedom
```{r}
(dfU <- n-1)
(dfR <- n-(1-1))
```

### $F$-test
 
The $F$-statistic is the amount of available fit that is actually achieved,

$$F=\frac{(RSS_{R}-RSS_{U})/(df_{R}-df_{U})}{RSS_U/df_U}=\frac{"Explained"}{"Not\,explained"}$$


```{r}
F <- (RSSR-RSSU)/(dfR-dfU)/(RSSU/(dfU))
p <- 1-pf(F,df1=dfR-dfU,df2=dfU)
sigmaU <- sqrt(RSSU/dfU)
sigmaR <- sqrt(RSSR/dfR)
print(data.frame(RSSR,RSSU,SSExplained=RSSR-RSSU,F,p,sigmaU,sigmaR),row.names=FALSE)
```


## log-Likelihood of both models
These are the log-Likelhoods of the model at MLE's (the maximum likelihood estimates).

```{r,eval=FALSE,echo=FALSE}
sU <- summary(modU)$sigma
sR <- summary(modC)$sigma
sum(log(dnorm(X,predict(modU),sqrt(sU^2*(n-1)/n))))
sum(log(dnorm(X,predict(modR),sR)))
```


```{r}
logLik(modU)
logLik(modR)
```

## AIC and BIC
Adding penalties for model complexity:
$$AIC=-2l+2p$$ with $l$ as the log-likelihood and $p$ the number of parameters in the model.
$$BIC=-2l+2\log(n)$$

```{r}
AIC(modR,modU)
BIC(modR,modU)
```


## Result
Smaller AIC and BIC (smaller negative penalized likelihoood) are better. We do NOT reject the constrained model in favor of the unconstrained model. 




