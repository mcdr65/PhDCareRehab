---
title: "$t$-Test"
author: "André Meichtry"
output:
  pdf_document:
    toc: yes
    toc_depth: '3'
  html_document:
    number_sections: yes
    self_contained: yes
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: no
---


```{r chunk_setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = T, message=F, warning=F, comment=NA, autodep=F,
                      eval=T, cache.rebuild=F, cache=F, R.options=list(width=120,digits=5,show.signif.stars=FALSE,scipen=5),fig.width=8, out.width="49%", fig.align = 'center', dev.args=list(bg = 'transparent'))
```

# One-sample $t$-test
One-sample $t$-tests look at just one parameter of interest $\mu$ (in
addition to $\sigma$ ) as
data-generation mechanism.

(see [shinyApp](https://rstudio-pubs.zhaw.ch/connect/#/apps/9/access)).


## Statistical model

+ $Y_{i}=\mu+\epsilon_{i}; \quad \epsilon_{i}
  \overset{i.i.d.}{\sim} N(0,\sigma^2)$ (or with $\epsilon_i$  i.i.d. and “large” sample size), $\quad i=1,...,n$
	  
+ $Y_{i} \overset{i.i.d.}{\sim} N(\mu,\sigma^2)$ (equivalent)


This is the simplest example of
$Y_i=\beta_1+\beta_2x_{i2}+\cdots+\beta_p
x_{ip}+\epsilon_i$, the **only intercept model** with $\beta_1=\mu$ (see [IntroLinearModels](https://home.zhaw.ch/~mcdr/LinearModelIntro.pdf)).


## Intro in sample size calculation 
Let us first ask the **how many do I need**-question. There are two
kinds of *sample size calculations*, the power approach and the
precision approach. 

Consider the problem
$H_0:\mu=\mu_0$ versus $H_1: \mu \neq \mu_0$.

### Power approach

```{r echo=FALSE,include=FALSE}
mu0<-0
mu1<-13
delta<-mu1-mu0
sigmahat<-22
```

A priori sample size calculation, see pages 77-78 AMT. Let $\beta$ be
the Type II error (The probability of **not** rejectig $H_0$ **if** a
specified alternative $H_1$ is true. $1-\beta$ is the probability of
the complement, of rejectig $H_0$ **if** a
specified alternative $H_1$ is true).

**Question**: What $n$ we
need to assure 

* to reject $H_0: \mu=\mu_0$ 
* with probability $1-\beta$
* with false-positive rate $\alpha$
* **if** the specified alternative $H_1:\mu=\mu_1$ is true?

Assume $\mu_0=0$, thus
$\delta=\mu_1-\mu_0=\mu_1$. One can show hat 

$$n\geq \frac{(z_{1-\alpha/2}+z_{1-\beta})^2}{(\delta/\sigma)^2}$$ 

The approximation $n \approx 8/(\delta/\sigma)^2$ holds for $\alpha=0.05$ and $\beta=0.2$.

Assume we estimate or "guess" $\sigma$ from another study with $\hat{\sigma}=`r sigmahat`$ and we want that
$H_0$ is rejected with probability 0.8 **if** $\mu_1=`r mu1`$ is true.
 
 
```{r }
(qnorm(.975)+qnorm(.8))^2/((mu1-mu0)/sigmahat)^2
```

In R, you can use the _exact_ version,
`power.t.test()`. Read the help file `help(power.t.test)`. You have to
specify $\delta$, $\sigma$, power and the type of test: 
```{r }
power.t.test(delta=mu1-mu0,sd=sigmahat,power=0.8,type="one.sample")
```



### Precision approach

In this approach, we do not need a Type II error. We have **not** to
specify the alternative, which **most often makes more sense**, since there
are **many (even infinite) options for the alternative** $\mu\neq \mu_0$.
We estimate the sample size by specifying the precision we want for
the estimation, that is, we specify a priori the maximal width of the $100\times (1-\alpha)$\% CI.

An approximative $100\times (1-\alpha)$\% CI is given by $\bar{x}\pm
  \delta$,  with
  $\delta=z_{1-\alpha/2}\frac{\sigma}{\sqrt{n}}$. Solving for $n$
  gives  
  
    
$$n\geq \frac{z^2_{1-\alpha/2}}{(\delta/\sigma)^2}$$
 
Using again  $\hat{\sigma}=`r sigmahat`$:
 
```{r }
delta<-c(2,3,4,5,8)
alpha<-0.05
data.frame(delta=delta,n=qnorm(1-alpha/2)^2*(sigmahat/delta)^2)
```


## Simulation of example data
Let us simulate some data from population with **assumed known
parameters** $\mu$ and $\sigma^2$. We simulate to understand the
emergence of data. Of course, in
reality, we do not know the data-generating process.


```{r }
set.seed(55)
mu<-10
sigma<-20
n<-40
Y<-rnorm(n,mu,sigma)
```


```{r }
hist(Y)
summary(Y)
```

## Analysis


### As one-sample $t$-Test

```{r }
t.test(Y)
```
### As linear model

This is equivalent with a linear model with only an intercept as parameter.


```{r }
summary(mod<-lm(Y~1))
```

Of course, the results are the same. The $t$-test **is** a simple linear model.

Estimation of $\sigma$ "by hand" (this quantity is given in the output above):

```{r }
sqrt(sum(mod$residuals^2)/(n-1)) 
```
Confidence interval for $\mu$:

```{r }
confint(mod)
```




### Small samples without normality

In the example above, the assumptions are met since we simulated data
from a normal distribution. In the absence of normality, this assumption of the $t$-test is not
met.  We can then perform a non-parametric test.

* The `wilcox.test` is the non-parametric alternative to the $t$-test.
* With the actual data,
results are similar since we have simulated from a normal
distribution.

```{r}
wilcox.test(Y,conf.int=TRUE) 
```








# Two-sample $t$-Test

(see [shinyApp](https://rstudio-pubs.zhaw.ch/connect/#/apps/7/access)).

## Statistical model

1. Means parameterization: 
	+ $Y_{ij}=\mu_i+\epsilon_{ij},  \quad i=1,2;  \quad j=1,...,n_i;
      \quad \epsilon_{ij} \overset{i.i.d.}{\sim} N(0,\sigma^2)$ 
    + $Y_{ij} \overset{i.i.d.}{\sim} N(\mu_i,\sigma^2)$ (equivalent)
2. Effects parameterization: 
	+ $Y_{ij}=\mu+\alpha_i+\epsilon_{ij}$
    + $Y_{ij} \overset{i.i.d.}{\sim} N(\mu+\alpha_i,\sigma^2)$ (equivalent)
3. In R regression language: 
   + $Y_i=\beta_0+\beta_1I_{group_i=2}+\epsilon_i$, with
	   + $i=1,...,n$
	   + $\beta_0=\mu_1,\quad\beta_1=\mu_2-\mu_1$
	   + $I_{group_i=2}=1,\,if\,group_i=2;\quad I_{group_i=2}=0, \,if\, group_i=1$ (="Dummy" variable)
   + (see [IntroLinearModels](https://home.zhaw.ch/~mcdr/LinearModelIntro.pdf)).


## Simulation of example data

We simulate to understand the emergence of data. Of course, in
reality, we do not know the data-generating process.

```{r}
n<-30 ##n per group
muInt<-21 ##True mean Int
muCont<-20 ##True mean Cont
Delta<-muInt-muCont ##True mean difference
Delta
sigma<-2 ##True standard deviation
set.seed(10)
dataInt <- rnorm(n=n,mean=muInt,sd=sigma) ##Sample from Int
dataCont <- rnorm(n=n,mean=muCont,sd=sigma) ##Sample from Cont
group<-gl(n=2,k=n,labels=c("Cont","Int"))##grouping variable
response<-c(dataCont,dataInt)##the outcome
mydata <- data.frame(response=response,group=group)##the data.frame
str(mydata)
head(mydata)
```


```{r}
summary(mydata)
by(mydata,mydata$group,summary)
boxplot(response~group,mydata)
```


## Analysis
### Hypothesis 
$H_0$: $\mu_I-\mu_C=0$  vs. $H_1:\mu_I-\mu_C\neq 0$
```{r}
test <- t.test(x=dataInt,y=dataCont,var.equal=TRUE)
test
```

#### Extract information from object **test**

```{r}
test$estimate ##Estimate of muInt and muCont
test$statistic ##t-statistic
test$stderr ##standard error 
((test$estimate[1]-test$estimate[2])-0)/test$stderr ##t-statistic "by hand"
test$p.value
```

### Other Hypotheses

* $H_0$: $\mu_I-\mu_C\leq 0$  vs. $H_1:\mu_I-\mu_C>0$
```{r}
test2 <- t.test(x=dataInt,y=dataCont,alternative="greater",var.equal=TRUE)
test2
```

* $H_0$: $\mu_I-\mu_C\leq -1$  vs. $H_1:\mu_I-\mu_C>-1$
```{r}
test3 <- t.test(x=dataInt,y=dataCont,mu=-1,alternative="greater",var.equal=TRUE)
test3
```

### Formula version

Back to the problem $H_0$: $\mu_I-\mu_C=0$  vs. $H_1:\mu_I-\mu_C\neq 0$


```{r}
test<- t.test(response~group,data=mydata,var.equal=TRUE)
test
```
### As linear model

```{r}
mod<-lm(response~group,data=mydata)
summary(mod)
confint(mod)
```

### What is the $F$-statistic

In the output of the model summary, there is an $F$-value. We will
introduce the $F$-statistic in the context of ANOVA.

The aim is to test the null that we can omit `group` from the model. In
this simple case, that is the same as the $t$-test for `group` ($\mu_I=\mu_C$). Very
generally, we use $F$-tests as tests in the context of **model comparison**.

```{r }
mod2<-lm(response~1,data=mydata) ## Intercept-only model
anova(mod,mod2) ## tests the null: "Effect of group is absent"
```

## Introduction in residual analysis

The **assumptions of linear models** are (see [IntroLinearModels](https://home.zhaw.ch/~mcdr/LinearModelIntro.pdf))

* $\epsilon_i$ are independent and identically distributed, i.i.d.
* $E(\epsilon_i)=0$ for all $i$.
* $Var(\epsilon_i)=\sigma^2$ constant for all $i$.

For the $t$-test, we have to assume **in addition**

* $\epsilon_i$ i.i.d. $\sim N(0,\sigma^2)$

In R, we can just call the `plot` function for the model object to
check the assumptions.

```{r}
plot(mod,which=c(1,2))
```


## Small samples without normality

In the example above, the assumptions are met since we simulated data
from a normal distribution. Again, in the absence of normality, this assumption of the $t$-test is not
met.  We can then perform a non-parametric test.

* The Mann-Whitney Test (`wilcox.test` in R) is the non-parametric
  alternative to the two-sample $t$-test.
* With the actual data,
results are similar since we have simulated from a normal
distribution.

```{r}
wilcox.test(dataInt,dataCont,conf.int=TRUE) 
```


# Equivalence testing 

## Philosophical background

Very often, it makes not much sense to test nulls such as
$H_0:\mu=0$ versus $H_1:\mu \neq 0$. Assume a theory predicts a range
for $\mu$, i.e. that $\mu$ lies in a region
$[-\epsilon,+\epsilon]$. 

You then have the following test situation:

$H_0:  \mu \leq
-\epsilon$ OR $\mu \geq +\epsilon$ versus $H_1: -\epsilon < \mu < \epsilon$, meaning that

* Null: The true parameter is outside a tolerance region, "irrelevance".
* Alternative: The true parameter is inside a tolerance region, "relevance". 
  
  **Rejecting $H_0$ now really means rejecting irrelevance (defined by
  the margins $\epsilon$).**

## Implementation

This analysis can be performed with TOST (Two One-sided $t$-Tests),
this is implemented in the package `TOSTER`. For the one-sample
situation, we have the function `TOSTone.raw()`.

```{r message=FALSE}
library(TOSTER)
## Test observed mean of 0.52 and standard deviation of 0.5 in sample of 300 participants
## against 0.5 given equivalence bounds in raw units of -0.1 and 0.1, with an alpha = 0.05.
TOSTone.raw(m=0.52,mu=0.5,sd=0.5,n=300,low_eqbound=-0.1, high_eqbound=0.1, alpha=0.05)
```

## Package TOSTER functions

If you need such analysis in the future, look at
  `help(package="TOSTER")` for other functions in the package, such as
  functions for sample size estimation.

